{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# A variable describes the weights connecting neurons between two layers of a feedforward neural network\n",
    "# In this case, weights is meant to be trainable, we will automatically compute and apply gradients to weights.\n",
    "weights = tf.Variable(tf.random_normal([300, 200], stddev=0.5), name=\"weights\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# If weights is not meant to be trainable, we may pass an optional flag when we call tf.Variable\n",
    "weigths1 = tf.Variable(tf.random_normal([300, 200], stddev=0.5), name=\"weights1\", trainable=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# In addition to tf.random_normal, there are several other methods to initialize a TensorFlow variable:\n",
    "# Common tensors from the Tensorflow API docs\n",
    "shape = [2,3]\n",
    "x1 = tf.zeros(shape, dtype=tf.float32, name=\"x1\")\n",
    "x2 = tf.ones(shape, dtype=tf.float32, name=\"x2\")\n",
    "x3 = tf.random_normal(shape, mean=0.0, stddev=1.0, dtype=tf.float32, seed=None, name=\"x3\")\n",
    "x4 = tf.truncated_normal(shape, mean=0.0, stddev=1.0, dtype=tf.float32, seed=None, name=\"x4\")\n",
    "x5 = tf.random_uniform(shape, minval=0, maxval=100, dtype=tf.float32, seed=None , name=\"x5\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TensorFlow variables have the folling three properties:\n",
    "- Variables must be *explicitly* initialized before a graph is used for the first time\n",
    "- We can use gradient methods to modify variables after each iteration as we search for a model's optimal parameter settings\n",
    "- We can save the values stored in variables to disk and restore them for later use"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When we call `tf.Variable`, the following **three operations are added** to the computation graph:\n",
    "- The operation producing the tensor we use to initialize our variable\n",
    "- The `tf.assign` operation, which is responsible for filling the variable with the initializing tensor prior to the variable's use (All such assign ops in the graph are triggered by running `tf.initialize_all_variables()`)\n",
    "- The variable operation, which holds the current value of the variable"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**TensorFlow Operations**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "On a high-level, TensorFlow *operations* represent abstract transformations that are applied to tensors in the computation graph. Operations may have attributes that may be supplied a priori or are inferred at runtime. For example, an attribute may serve to describe the expected types of the input (adding tensors of type float32 vs. int32). Just as variables are named, operations may also be supplied with an optional name attribute for easy reference into the computation graph."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "An operation consists of one or more *kernels*, which represent device-specific implementations. For example, an operation may have separate CPU and GPU kernels because it can be more efficiently expressed on a GPU. This is the case for many TensorFlow operations on matrices."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "** Placeholder Tensors **\n",
    "How we pass the input to our deep model during both train and test time. A variable is insufficient because it is only meant to be initialized once. We instead need a component that we populate every single time the computation graph is run."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "x = tf.placeholder(tf.float32, name=\"x\", shape=[None, 784]) # a mini-batch of data stored as float32\n",
    "W = tf.Variable(tf.random_uniform([784, 10], -1, 1), name=\"W\")\n",
    "multiply = tf.matmul(x, W)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "While we could instead multiply each data sample separately by *W*, expressing a full mini-batch as a tensor allows us to compute the results for all the data samples in parallel."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Just as variables need to be initialized the first time the computation graph is built, placeholders need to be filled every time the computation graph (or a subgraph) is run. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "** Sessions in TensorFlow **"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A TensorFlow program interacts with a computation graph using a *session*. The TensorFlow session is responsible for building the initial graph, can be used to initialize all variables appropriately, and to run the computational graph."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We then initialize the variables as required by using the session variable to run the initialization operation in *session.run(init_op)*."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will explore two more major concepts in building and maintaining computational graphs.\n",
    "** Variable Scopes ** and ** Sharing Variables **\n",
    "Building complex models often requires re-using and sharing large sets of variables that we'll want to instantiate together in one place.\n",
    "In many cases, we don't want to create a copy, but instead, we want to reuse the model and its variables. It turns out, in this case, we shouldn't be using `tf.Variable`. Instead, we should using a more advanced naming scheme that takes advantage of TensorFlow's variable scoping."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TensorFlow's variable scoping mechanisms are largely controlled by two functions:\n",
    "- *tf.get_variable(name,shape,initializer)*: checks if a variable with this name exists, retrieves the variable if it does, creates it using the shape and initializer if it doesn't.\n",
    "- *tf.variable_scope(scope_name)*: manages the namespace and determines the scope in which *tf.get_variable* operates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Unlike *tf.Variable*, the *tf.get_variable* command checks that a variable of the given name hasn't already been instantiated. By default, sharing is not allowed (just to be safe!), but if you want to enable sharing within a variable scope, we can say so explictly:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Enable *sharing* within a variable scope:\n",
    "- with tf.variable_scope(\"shared_variables\") as scope:\n",
    "    ...\n",
    "    scope.reuse_variables()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "** Managing Models over the CPU and GPU **"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TensorFlow allows us to utilize multiple computing devices if we so desire to build and train our models. Supported devices are represented by string ID's and normally consist of the following:\n",
    "- \"/cpu:0\": The cpu of our machine.\n",
    "- \"/gpu:0\": The first gpu of our machine, if it has one.\n",
    "- \"/gpu:1\": The second gpu of our machine, if it has one."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To inspect which devices are used by the computational graph, we can initialize our TensorFlow session with the *log_device_placement* set to *True*:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "sess = tf.Session(config=tf.ConfigProto(log_device_placement=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[  5.],\n",
       "       [ 11.]], dtype=float32)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "with tf.device('/gpu:2'):\n",
    "    a = tf.constant([1.0, 2.0, 3.0, 4.0], shape=[2,2], name='a')\n",
    "    b = tf.constant([1.0, 2.0], shape=[2,1], name='b')\n",
    "    c = tf.matmul(a, b)\n",
    "sess = tf.Session(config=tf.ConfigProto(allow_soft_placement=True, log_device_placement=True))\n",
    "sess.run(c)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "c = []\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.4.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
